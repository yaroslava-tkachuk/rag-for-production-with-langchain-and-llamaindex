{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONRxYIGDHNr7"
      },
      "source": [
        "# Deep Memory trained on Syntethic Queries improves recall@10 by +20%\n",
        "\n",
        "You need to have labelled data (query and relevance pairs) for training deep memory. However it is sometimes hard to obtain labelled data when you start fresh.\n",
        "\n",
        "In this tutorial we will take an existing dataset and generate queries using GPT to train Deep Memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEgfW2vHPDK"
      },
      "source": [
        "## 0. Setup packages and credentials\n",
        "Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCZJ5PXlGMu6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q llama-index deeplake openai cohere llama-index-readers-wikipedia wikipedia llama-index-vector-stores-deeplake python-dotenv langchain-openai deeplake==3.9.27"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRYFBOBMHG16"
      },
      "source": [
        "Setup Activeloop and OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "re7qIcjoGrdm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "load_dotenv(\"../.env\")\n",
        "assert os.getenv(\"OPENAI_API_KEY\")\n",
        "assert os.getenv(\"ACTIVELOOP_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVACLdPZHqwW"
      },
      "source": [
        "## 1. Load the dataset and create a Deep Lake vector store\n",
        "\n",
        "We are going to use GPT3.5 to generate questions based on the context provided by a chunk test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NCMGoopuG0t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 75042  100 75042    0     0   251k      0 --:--:-- --:--:-- --:--:--  251k\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p \"data/paul_graham/\"\n",
        "!curl \"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\" -o \"data/paul_graham/paul_graham_essay.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h2Q8HcqxuJiw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Documents: 1\n",
            "Number of nodes: 64 with the current chunk size of 512\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
        "for idx, node in enumerate(nodes):\n",
        "    node.id_ = f\"node_{idx}\"\n",
        "\n",
        "print(f\"Number of Documents: {len(documents)}\")\n",
        "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UNbSMwL8OY_H"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/coder/Desktop/dev/rag_for_production_activeloop/rag-for-production-with-langchain-and-llamaindex/.venv/lib/python3.12/site-packages/humbug/report.py:47: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources  # type: ignore\n",
            "/Users/coder/Desktop/dev/rag_for_production_activeloop/rag-for-production-with-langchain-and-llamaindex/.venv/lib/python3.12/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (4.3.1) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n",
            "/Users/coder/Desktop/dev/rag_for_production_activeloop/rag-for-production-with-langchain-and-llamaindex/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Generating embeddings:   0%|          | 0/64 [00:00<?, ?it/s]2025-09-04 12:02:40,995 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "Generating embeddings: 100%|██████████| 64/64 [00:01<00:00, 43.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 64/64 [00:00<00:00, 709.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset(path='./data/paul_graham/deep_lake_db', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            "   text       text      (64, 1)      str     None   \n",
            " metadata     json      (64, 1)      str     None   \n",
            " embedding  embedding  (64, 1536)  float32   None   \n",
            "    id        text      (64, 1)      str     None   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
        "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "\n",
        "# Create a DeepLakeVectorStore locally to store the vectors\n",
        "dataset_path = \"./data/paul_graham/deep_lake_db\"\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
        "\n",
        "# LLM that will answer questions with the retrieved context\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex(\n",
        "    nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model,\n",
        "    llm=llm,\n",
        "    show_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKiyo5SpuxYD"
      },
      "source": [
        "Now let's upload the local Vectore Store to Active Loop's platform and then convert it into a managed database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yW2imseOuLG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying dataset: 96%|█████████▋| 27/28 [00:08<00:00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/yaroslava/optimization_paul_graham_0\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Copying dataset: 96%|█████████▋| 27/28 [00:15<00:00\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/yaroslava/optimization_paul_graham_managed_0\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset(path='hub://yaroslava/optimization_paul_graham_managed_0', tensors=['embedding', 'id', 'metadata', 'text'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import deeplake\n",
        "\n",
        "\n",
        "local = \"./data/paul_graham/deep_lake_db\"\n",
        "hub_path = \"hub://yaroslava/optimization_paul_graham_0\"\n",
        "hub_managed_path = \"hub://yaroslava/optimization_paul_graham_managed_0\"\n",
        "\n",
        "\n",
        "# First upload our local vector store\n",
        "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
        "# Create a managed vector store under a different name\n",
        "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A-WROoEOz38"
      },
      "source": [
        "## 2. Generate a dataset of Queries and Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t_1d6IRqvvUU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Lake Dataset in hub://yaroslava/optimization_paul_graham_managed_0 already exists, loading from the storage\n",
            "64\n"
          ]
        }
      ],
      "source": [
        "# fetch dataset docs and ids if they exist (optional you can also ingest)\n",
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True)\n",
        "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)[\"value\"]\n",
        "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)[\"value\"]\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'node_0'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0]\n",
        "ids[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c7pjHtslc9Ao"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "\n",
        "def generate_question(text: str) -> str:\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
        "                        You make sure the question can be answered by the text.\"},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        question_string = \"No question generated\"\n",
        "        return question_string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "coaUWAp4ISyw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/40 [00:00<?, ?it/s]2025-09-04 12:23:45,959 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "  2%|▎         | 1/40 [00:01<01:14,  1.91s/it]2025-09-04 12:23:47,747 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "  5%|▌         | 2/40 [00:03<01:09,  1.83s/it]2025-09-04 12:23:49,898 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "  8%|▊         | 3/40 [00:05<01:13,  1.98s/it]2025-09-04 12:23:51,912 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 10%|█         | 4/40 [00:07<01:11,  1.99s/it]2025-09-04 12:23:53,418 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 12%|█▎        | 5/40 [00:09<01:03,  1.82s/it]2025-09-04 12:23:55,324 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 15%|█▌        | 6/40 [00:11<01:02,  1.85s/it]2025-09-04 12:23:56,654 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 18%|█▊        | 7/40 [00:12<00:55,  1.68s/it]2025-09-04 12:23:59,316 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 20%|██        | 8/40 [00:15<01:03,  1.99s/it]2025-09-04 12:24:00,303 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 22%|██▎       | 9/40 [00:16<00:51,  1.68s/it]2025-09-04 12:24:01,086 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 25%|██▌       | 10/40 [00:17<00:42,  1.40s/it]2025-09-04 12:24:01,877 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 28%|██▊       | 11/40 [00:17<00:35,  1.21s/it]2025-09-04 12:24:02,696 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 30%|███       | 12/40 [00:18<00:30,  1.09s/it]2025-09-04 12:24:03,414 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 32%|███▎      | 13/40 [00:19<00:26,  1.02it/s]2025-09-04 12:24:04,540 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 35%|███▌      | 14/40 [00:20<00:26,  1.02s/it]2025-09-04 12:24:06,997 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 38%|███▊      | 15/40 [00:22<00:36,  1.46s/it]2025-09-04 12:24:08,944 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 40%|████      | 16/40 [00:24<00:38,  1.60s/it]2025-09-04 12:24:10,170 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 42%|████▎     | 17/40 [00:26<00:34,  1.49s/it]2025-09-04 12:24:12,093 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 45%|████▌     | 18/40 [00:28<00:35,  1.62s/it]2025-09-04 12:24:12,757 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 48%|████▊     | 19/40 [00:28<00:28,  1.34s/it]2025-09-04 12:24:13,808 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 50%|█████     | 20/40 [00:29<00:24,  1.25s/it]2025-09-04 12:24:14,985 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 52%|█████▎    | 21/40 [00:30<00:23,  1.23s/it]2025-09-04 12:24:16,828 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 55%|█████▌    | 22/40 [00:32<00:25,  1.41s/it]2025-09-04 12:24:17,647 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 57%|█████▊    | 23/40 [00:33<00:20,  1.23s/it]2025-09-04 12:24:18,670 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 60%|██████    | 24/40 [00:34<00:18,  1.17s/it]2025-09-04 12:24:19,390 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 62%|██████▎   | 25/40 [00:35<00:15,  1.04s/it]2025-09-04 12:24:20,001 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 65%|██████▌   | 26/40 [00:35<00:12,  1.10it/s]2025-09-04 12:24:21,845 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 68%|██████▊   | 27/40 [00:37<00:15,  1.19s/it]2025-09-04 12:24:23,893 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 70%|███████   | 28/40 [00:39<00:17,  1.45s/it]2025-09-04 12:24:25,121 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 72%|███████▎  | 29/40 [00:41<00:15,  1.38s/it]2025-09-04 12:24:27,001 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 75%|███████▌  | 30/40 [00:42<00:15,  1.53s/it]2025-09-04 12:24:29,013 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 78%|███████▊  | 31/40 [00:44<00:15,  1.67s/it]2025-09-04 12:24:29,831 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 80%|████████  | 32/40 [00:45<00:11,  1.42s/it]2025-09-04 12:24:30,995 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 82%|████████▎ | 33/40 [00:46<00:09,  1.34s/it]2025-09-04 12:24:32,047 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 85%|████████▌ | 34/40 [00:47<00:07,  1.26s/it]2025-09-04 12:24:33,214 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 88%|████████▊ | 35/40 [00:49<00:06,  1.23s/it]2025-09-04 12:24:33,665 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 90%|█████████ | 36/40 [00:49<00:03,  1.01it/s]2025-09-04 12:24:35,565 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 92%|█████████▎| 37/40 [00:51<00:03,  1.27s/it]2025-09-04 12:24:39,151 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 95%|█████████▌| 38/40 [00:55<00:03,  1.96s/it]2025-09-04 12:24:41,038 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            " 98%|█████████▊| 39/40 [00:56<00:01,  1.94s/it]2025-09-04 12:24:43,146 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "100%|██████████| 40/40 [00:59<00:00,  1.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40\n",
            "What led the founders of Y Combinator to create the Summer Founders Program?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate_queries(docs: list[str], ids: list[str], n: int) -> tuple[list[str], list[list[tuple[str, int]]]]:\n",
        "    questions = []\n",
        "    relevances = []\n",
        "    pbar = tqdm(total=n)\n",
        "    while len(questions) < n:\n",
        "        # 1. randomly draw a piece of text and relevance id\n",
        "        r = random.randint(0, len(docs)-1)\n",
        "        text, label = docs[r], ids[r]\n",
        "\n",
        "        # 2. generate queries and assign and relevance id\n",
        "        generated_qs = [generate_question(text)]\n",
        "        if generated_qs == [\"No question generated\"]:\n",
        "            print(\"No question generated\")\n",
        "            continue\n",
        "\n",
        "        questions.extend(generated_qs)\n",
        "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
        "        pbar.update(len(generated_qs))\n",
        "\n",
        "    return questions[:n], relevances[:n]\n",
        "\n",
        "# Here we choose to generate 40 questions\n",
        "questions, relevances = generate_queries(docs, ids, n=40)\n",
        "print(len(questions))\n",
        "print(questions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKETptufMlDb"
      },
      "source": [
        "## 3. Train Deep Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Qhx2PjztMmdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting DeepMemory training job\n",
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing training data for deepmemory:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating 40 embeddings in 1 batches of size 40::   0%|          | 0/1 [00:00<?, ?it/s]2025-09-04 12:25:32,914 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "Creating 40 embeddings in 1 batches of size 40:: 100%|██████████| 1/1 [00:09<00:00,  9.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepMemory training job started. Job ID: 68b9696b2fed3f602c85a7e6\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "\n",
        "openai_embeddings = OpenAIEmbeddings()\n",
        "\n",
        "job_id = db.vectorstore.deep_memory.train(\n",
        "    queries=questions,\n",
        "    relevance=relevances,\n",
        "    embedding_function=openai_embeddings.embed_documents,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9XneeHJdOE0g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/yaroslava/optimization_paul_graham_managed_0\n",
            "--------------------------------------------------------------\n",
            "|                  68b9696b2fed3f602c85a7e6                  |\n",
            "--------------------------------------------------------------\n",
            "| status                     | pending                       |\n",
            "--------------------------------------------------------------\n",
            "| progress                   | None                          |\n",
            "--------------------------------------------------------------\n",
            "| results                    | not available yet             |\n",
            "--------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "db.vectorstore.deep_memory.status(\"68b9696b2fed3f602c85a7e6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFoi25nXXj4L"
      },
      "source": [
        "Wait until training status becomes completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znNn87saMnr1"
      },
      "source": [
        "## 4. Evaluate Deep Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vIbzAQSGD9"
      },
      "source": [
        "### 4.1 Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgi-zdwuSBNj"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms import OpenAI\n",
        "\n",
        "\n",
        "query = \"What are the main things Paul worked on before college?\"\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True)\n",
        "vector_index = VectorStoreIndex.from_vector_store(db, service_context=service_context, storage_context=storage_context, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ht4F2fR97p"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": False})\n",
        "response_vector = query_engine.query(query)\n",
        "print(response_vector.response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxlN0VrER-ap"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": True})\n",
        "response_vector = query_engine.query(query)\n",
        "print(response_vector.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZBoA8YqSI4b"
      },
      "source": [
        "### 4.2 Quantitative Evaluation on Synthetically generated queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd65U190Mp9v"
      },
      "outputs": [],
      "source": [
        "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXJYCiwNPbOL"
      },
      "outputs": [],
      "source": [
        "recalls = db.vectorstore.deep_memory.evaluate(\n",
        "    queries=validation_questions,\n",
        "    relevance=validation_relevances,\n",
        "    embedding_function=openai_embeddings.embed_documents,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
