{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet tiktoken langchain-openai python-dotenv datasets langchain deeplake beautifulsoup4 html2text ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ea70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "assert os.getenv(\"OPENAI_API_KEY\")\n",
    "assert os.getenv(\"ACTIVELOOP_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab20cf4",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76535069",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embeddings = OpenAIEmbeddings()\n",
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://yaroslava/deeplake-docs-deepmemory\",\n",
    "    embedding=openai_embeddings,\n",
    "    runtime={\"tensor_db\": True},\n",
    "    token=os.getenv(\"ACTIVELOOP_TOKEN\"),\n",
    "    overwrite=True,\n",
    "    read_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0792ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_all_links(url: str) -> list[str]:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page: {url}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Finding all 'a' tags which typically contain href attribute for links\n",
    "    links = [\n",
    "        urljoin(url, a[\"href\"]) for a in soup.find_all(\"a\", href=True) if a[\"href\"]\n",
    "    ]\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "base_url = \"https://docs.deeplake.ai/4.3/\"\n",
    "all_links = get_all_links(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53764453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.async_html import AsyncHtmlLoader\n",
    "\n",
    "\n",
    "loader = AsyncHtmlLoader(all_links)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "chunk_size = 4096\n",
    "docs_new = []\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    ")\n",
    "\n",
    "for doc in docs_transformed:\n",
    "    if len(doc.page_content) < chunk_size:\n",
    "        docs_new.append(doc)\n",
    "    else:\n",
    "        docs = text_splitter.create_documents([doc.page_content])\n",
    "        docs_new.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad7ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.add_documents(docs_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac362ed",
   "metadata": {},
   "source": [
    "## 2. Generating synthetic queries and training Deep Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions import (\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset docs and ids if they exist (optional you can also ingest)\n",
    "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)[\"value\"]\n",
    "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we pass in a model explicitly, we need to make sure it supports the OpenAI function-calling API.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    question: str = Field(..., description=\"Questions about text\")\n",
    "\n",
    "\n",
    "prompt_msgs = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a world class expert for generating questions based on provided context. \\\n",
    "                You make sure the question can be answered by the text.\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Use the given text to generate a question from the following input: {input}\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate(messages=prompt_msgs)\n",
    "chain = create_structured_output_chain(Questions, llm, prompt, verbose=True)\n",
    "\n",
    "text = \"# Understanding Hallucinations and Bias ## **Introduction** In this lesson, we'll cover the concept of **hallucinations** in LLMs, highlighting their influence on AI applications and demonstrating how to mitigate them using techniques like the retriever's architectures. We'll also explore **bias** within LLMs with examples.\"\n",
    "questions = chain.run(input=text)\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef7f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_queries(\n",
    "    docs: list[str],\n",
    "    ids: list[str],\n",
    "    n: int = 100\n",
    ") -> tuple[list[str], list[list[tuple[str, int]]]]:\n",
    "    questions = []\n",
    "    relevances = []\n",
    "    pbar = tqdm(total=n)\n",
    "    while len(questions) < n:\n",
    "        # 1. randomly draw a piece of text and relevance id\n",
    "        r = random.randint(0, len(docs) - 1)\n",
    "        text, label = docs[r], ids[r]\n",
    "\n",
    "        # 2. generate queries and assign and relevance id\n",
    "        generated_qs = [chain.run(input=text).question]\n",
    "        questions.extend(generated_qs)\n",
    "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
    "        pbar.update(len(generated_qs))\n",
    "        if len(questions) % 10 == 0:\n",
    "            print(f\"q: {len(questions)}\")\n",
    "    return questions[:n], relevances[:n]\n",
    "\n",
    "\n",
    "chain = create_structured_output_chain(Questions, llm, prompt, verbose=False)\n",
    "questions, relevances = generate_queries(docs, ids, n=200)\n",
    "\n",
    "train_questions, train_relevances = questions[:100], relevances[:100]\n",
    "test_questions, test_relevances = questions[100:], relevances[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = db.vectorstore.deep_memory.train(\n",
    "    queries=train_questions,\n",
    "    relevance=train_relevances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93beb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.vectorstore.deep_memory.status(\"6538939ca0b69a9ca45c528c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69133280",
   "metadata": {},
   "source": [
    "## 3. Evaluating Deep Memory performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76b9a7c",
   "metadata": {},
   "source": [
    "### 3.1 Deep Memory evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = db.vectorstore.deep_memory.evaluate(\n",
    "    queries=test_questions,\n",
    "    relevance=test_relevances,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bffa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Deep Memory + RAGas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.langchain import RagasEvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_relevance_to_ground_truth(\n",
    "    docs: list[str],\n",
    "    relevance: list[list[tuple[int, int]]]\n",
    ") -> list[list[str]]:\n",
    "    ground_truths = []\n",
    "    for rel in relevance:\n",
    "        ground_truth = []\n",
    "        for doc_id, _ in rel:\n",
    "            ground_truth.append(docs[doc_id])\n",
    "        ground_truths.append(ground_truth)\n",
    "    return ground_truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4f711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = convert_relevance_to_ground_truth(docs, test_relevances)\n",
    "\n",
    "for deep_memory in [False, True]:\n",
    "    print(\"\\nEvaluating with deep_memory =\", deep_memory)\n",
    "    print(\"===================================\")\n",
    "\n",
    "    retriever = db.as_retriever()\n",
    "    retriever.search_kwargs[\"deep_memory\"] = deep_memory\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=ChatOpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"context_recall_score\": 0,\n",
    "    }\n",
    "\n",
    "    eval_chains = {m.name: RagasEvaluatorChain(metric=m) for m in [context_recall]}\n",
    "\n",
    "    for question, ground_truth in zip(test_questions, ground_truths):\n",
    "        result = qa_chain({\"query\": question})\n",
    "        result[\"ground_truths\"] = ground_truth\n",
    "        for name, eval_chain in eval_chains.items():\n",
    "            score_name = f\"{name}_score\"\n",
    "            metrics[score_name] += eval_chain(result)[score_name]\n",
    "\n",
    "    for metric in metrics:\n",
    "        metrics[metric] /= len(test_questions)\n",
    "        print(f\"{metric}: {metrics[metric]}\")\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa228d",
   "metadata": {},
   "source": [
    "## 3.3 Deep Memory Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce40441",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"deep_memory\"] = True\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "query = \"The base type of the video_seq tensor.\"\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4\"), chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd64c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs[\"deep_memory\"] = False\n",
    "retriever.search_kwargs[\"k\"] = 10\n",
    "\n",
    "query = \"The base type of the video_seq tensor.\"\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(model=\"gpt-4\"), chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cc788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
